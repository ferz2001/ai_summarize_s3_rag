import os
import subprocess
import time
from pathlib import Path
import tempfile
import shutil
import httpx

from faster_whisper import WhisperModel
from core.config import config
from services.qdrant_service import QdrantService


def extract_audio(video_path: str, audio_path: str) -> None:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–≤—É–∫–æ–≤—É—é –¥–æ—Ä–æ–∂–∫—É –≤ 16kHz –º–æ–Ω–æ WAV —Å –ø–æ–º–æ—â—å—é ffmpeg."""
    command = [
        "ffmpeg",
        "-i",
        video_path,
        "-vn",
        "-acodec",
        "pcm_s16le",
        "-ar",
        "16000",
        "-ac",
        "1",
        audio_path,
        "-y",
    ]
    subprocess.run(command, check=True)


def transcribe_audio(audio_path: str, language: str = "ru") -> str:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é Whisper.
    Args:
        audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É.
        language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ.
    Returns:
        –¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
    """
    print(f"üó£Ô∏è –ù–∞—á–∏–Ω–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∞—É–¥–∏–æ: {audio_path}")
    start_time = time.time()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Whisper
    model = WhisperModel("small", device="cpu", compute_type="int8")
    
    segments, info = model.transcribe(
        audio_path, 
        language=language,
        condition_on_previous_text=False,
        word_timestamps=False
    )
    
    transcript = " ".join([segment.text for segment in segments])
    
    print(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {time.time() - start_time:.1f}—Å")
    print(f"üìù –ü–æ–ª—É—á–µ–Ω–æ {len(transcript)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞")
    
    return transcript.strip()


async def summarize_text(text: str, model: str = None) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –ø–µ—Ä–µ—Å–∫–∞–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    prompt = (
        "–°–¥–µ–ª–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π, –Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∂–∞—Ç—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
        "–°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–∑–±–µ–≥–∞–π –ø–æ—Ç–µ—Ä–∏ –≤–∞–∂–Ω—ã—Ö —Å–º—ã—Å–ª–æ–≤. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç <think>. "
        "–í—ã–¥–∞–π —Å–≤—è–∑–Ω—ã–π, –ª–æ–≥–∏—á–Ω—ã–π –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–º–µ—Å—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞:\n\n"
        f"{text}"
    )
    if config.USE_LOCAL_MODELS:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –±–µ–∑ fallback
        from services.local_models_service import LocalModelsService
        local_models = LocalModelsService()
        summary = await local_models.chat_completion([
            {
                "role": "system",
                "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–æ–∫. –°–æ–∑–¥–∞–≤–∞–π –∫—Ä–∞—Ç–∫–∏–µ, –Ω–æ –ø–æ–ª–Ω—ã–µ –ø–µ—Ä–µ—Å–∫–∞–∑—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
            },
            {
                "role": "user", 
                "content": prompt
            }
        ], model)
        print(summary)
        return summary
    else:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {config.OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 2000,
                    "temperature": 0.7
                }
            )
            response.raise_for_status()
            content = response.json()["choices"][0]["message"]["content"]
            print(content)
            return content


async def summarize_audio(audio_path: str, language: str = "ru", save_to_qdrant: bool = True) -> str:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã–∂–∏–º–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    Args:
        audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É.
        language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ.
        save_to_qdrant: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≤—ã–∂–∏–º–∫—É –≤ Qdrant.
    Returns:
        –í—ã–∂–∏–º–∫–∞ –∏–∑ –∞—É–¥–∏–æ.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤—ã–∂–∏–º–∫–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
    if save_to_qdrant:
        qdrant_service = QdrantService()
        existing = await qdrant_service.check_file_exists(audio_path)
        if existing:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {Path(audio_path).name}")
            print(f"üìÖ –°–æ–∑–¥–∞–Ω–∞: {existing['created_at']}")
            return existing['summary']
    
    print("üó£Ô∏è  –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é‚Ä¶")
    transcript = transcribe_audio(audio_path, language=language)
    print("\nüîä –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–æ–±—Ä–µ–∑–∞–Ω–æ):")
    print(transcript)

    print("\nüìù –°–æ–∑–¥–∞—é –≤—ã–∂–∏–º–∫—É —Å –ø–æ–º–æ—â—å—é –ò–ò‚Ä¶")
    summary = await summarize_text(transcript)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
    if save_to_qdrant:
        try:
            metadata = {
                "language": language,
                "transcript_length": len(transcript),
                "model": config.LOCAL_CHAT_MODEL if config.USE_LOCAL_MODELS else "gpt-4o-mini"
            }
            await qdrant_service.save_summary(audio_path, summary, "audio", metadata)
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Qdrant: {e}")
    
    return summary


async def summarize_video(video_path: str, language: str = "ru", save_to_qdrant: bool = True) -> str:
    """
    –°–æ–∑–¥–∞–µ—Ç –≤—ã–∂–∏–º–∫—É –∏–∑ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞.
    Args:
        video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É.
        language: –Ø–∑—ã–∫ –∞—É–¥–∏–æ –≤ –≤–∏–¥–µ–æ.
        save_to_qdrant: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≤—ã–∂–∏–º–∫—É –≤ Qdrant.
    Returns:
        –í—ã–∂–∏–º–∫–∞ –∏–∑ –≤–∏–¥–µ–æ.
    """
    start_time = time.time()
    if not Path(video_path).exists():
        raise FileNotFoundError(f"–§–∞–π–ª {video_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤—ã–∂–∏–º–∫–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
    if save_to_qdrant:
        qdrant_service = QdrantService()
        existing = await qdrant_service.check_file_exists(video_path)
        if existing:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–ª—è —Ñ–∞–π–ª–∞ {Path(video_path).name}")
            print(f"üìÖ –°–æ–∑–¥–∞–Ω–∞: {existing['created_at']}")
            return existing['summary']

    with Path(config.TMP_AUDIO) as tmp_audio_path:
        try:
            print("üé¨ –ò–∑–≤–ª–µ–∫–∞—é –∞—É–¥–∏–æ‚Ä¶")
            extract_audio(video_path, str(tmp_audio_path))

            # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –∞—É–¥–∏–æ-–≤—ã–∂–∏–º–∫—É –≤ Qdrant, —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –≤–∏–¥–µ–æ-–≤—ã–∂–∏–º–∫—É
            summary = await summarize_audio(str(tmp_audio_path), language=language, save_to_qdrant=False)

            print("\nüìå –ò—Ç–æ–≥–æ–≤—ã–π –ø–µ—Ä–µ—Å–∫–∞–∑:")
            print(summary)

            elapsed = time.time() - start_time
            print(f"\n‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed:.1f} —Å–µ–∫—É–Ω–¥")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–¥–µ–æ-–≤—ã–∂–∏–º–∫—É –≤ Qdrant
            if save_to_qdrant:
                try:
                    metadata = {
                        "language": language,
                        "processing_time": elapsed,
                        "model": config.LOCAL_CHAT_MODEL if config.USE_LOCAL_MODELS else "gpt-4o-mini"
                    }
                    await qdrant_service.save_summary(video_path, summary, "video", metadata)
                except Exception as e:
                    print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Qdrant: {e}")
            
            return summary
        finally:
            if tmp_audio_path.exists():
                os.remove(tmp_audio_path)


async def search_summaries(query: str, limit: int = 5, min_score: float = 0.3) -> str:
    """
    –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤—ã–∂–∏–º–∫–∏ –≤ Qdrant –ø–æ –∑–∞–ø—Ä–æ—Å—É.
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0).
    Returns:
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤—ã–∂–∏–º–æ–∫.
    """
    try:
        qdrant_service = QdrantService()
        results = await qdrant_service.search_similar_summaries(query, limit=limit, min_score=min_score)
        
        if not results:
            return f"ü§∑ –ü–æ—Ö–æ–∂–∏–µ –≤—ã–∂–∏–º–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–º–∏–Ω. —Å—Ö–æ–∂–µ—Å—Ç—å: {min_score:.1f})."
        
        output = f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ—Ö–æ–∂–∏—Ö –≤—ã–∂–∏–º–æ–∫:\n\n"
        
        for i, result in enumerate(results, 1):
            score = result['score']
            file_name = result['file_name']
            file_type = result['file_type']
            created_at = result['created_at'][:10]  # –¢–æ–ª—å–∫–æ –¥–∞—Ç–∞
            summary_preview = result['summary'][:200] + "..." if len(result['summary']) > 200 else result['summary']
            chunks_count = result.get('chunks_count', 1)
            is_chunked = result.get('is_chunked', False)
            
            output += f"{i}. üìÅ {file_name} ({file_type})\n"
            output += f"   üìä –°—Ö–æ–∂–µ—Å—Ç—å: {score:.3f}\n"
            output += f"   üìÖ –°–æ–∑–¥–∞–Ω–∞: {created_at}\n"
            
            if is_chunked:
                output += f"   üß© –ß–∞–Ω–∫–æ–≤: {chunks_count}\n"
            
            output += f"   üìù –í—ã–∂–∏–º–∫–∞: {summary_preview}\n\n"
        
        return output
        
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}"


async def summarize_text_and_save(text: str, file_path: str = "manual_input", model: str = None) -> str:
    """
    –°–æ–∑–¥–∞–µ—Ç –≤—ã–∂–∏–º–∫—É –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ Qdrant.
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (–∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞).
        model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–∫–∏.
    Returns:
        –í—ã–∂–∏–º–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    """
    summary = await summarize_text(text, model)

    try:
        qdrant_service = QdrantService()
        metadata = {
            "text_length": len(text),
            "model": config.LOCAL_CHAT_MODEL if config.USE_LOCAL_MODELS else "gpt-4o-mini",
            "source": "manual_input"
        }
        await qdrant_service.save_summary(file_path, summary, "text", metadata)
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Qdrant: {e}")
    
    return summary
