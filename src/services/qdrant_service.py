import hashlib
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import re
import httpx

from qdrant_client import AsyncQdrantClient
from qdrant_client.models import Distance, PointStruct, VectorParams


class QdrantService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qdrant –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""
    
    def __init__(self, host: str = None, port: int = None, collection_name: str = None):
        from core.config import config
        
        self.client = AsyncQdrantClient(
            host=host or config.QDRANT_HOST,
            port=port or config.QDRANT_PORT
        )
        self.collection_name = collection_name or config.QDRANT_COLLECTION_NAME
    
    async def _ensure_collection_exists(self):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        from core.config import config
        
        try:
            collections = await self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –º–æ–¥–µ–ª–∏
            vector_size = 1024 if config.USE_LOCAL_MODELS else 1536
            
            if self.collection_name not in collection_names:
                print(f"üîß –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}' –≤ Qdrant (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {vector_size})...")
                await self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
                )
                print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                collection_info = await self.client.get_collection(self.collection_name)
                existing_size = collection_info.config.params.vectors.size
                
                if existing_size != vector_size:
                    print(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –º–æ–¥–µ–ª–∏ ({existing_size} != {vector_size})")
                    print(f"üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}' —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é...")
                    
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
                    await self.client.delete_collection(self.collection_name)
                    
                    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é
                    await self.client.create_collection(
                        collection_name=self.collection_name,
                        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
                    )
                    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é")
                else:
                    print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {vector_size})")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π: {e}")
            raise
    
    async def _get_text_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ OpenAI."""
        from core.config import config
        
        if config.USE_LOCAL_MODELS:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –±–µ–∑ fallback
            from services.local_models_service import LocalModelsService
            
            local_models = LocalModelsService()
            embeddings = await local_models.get_embeddings([text])
            return embeddings[0]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.openai.com/v1/embeddings",
                    headers={
                        "Authorization": f"Bearer {config.OPENAI_API_KEY}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "input": text,
                        "model": "text-embedding-3-small"
                    }
                )
                response.raise_for_status()
                return response.json()["data"][0]["embedding"]
    
    def _generate_file_hash(self, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
        file_path_obj = Path(file_path)
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if file_path_obj.exists():
            stat = file_path_obj.stat()
            hash_string = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
        else:
            # –î–ª—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, manual_input) –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ –ø—É—Ç—å
            hash_string = f"{file_path}_virtual"
        
        return hashlib.md5(hash_string.encode()).hexdigest()
    
    def _split_text_into_chunks(self, text: str, max_chunk_size: int = 400, overlap: int = 50) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            max_chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            test_chunk = current_chunk + (". " if current_chunk else "") + sentence
            
            if len(test_chunk) <= max_chunk_size:
                current_chunk = test_chunk
            else:
                # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –Ω–µ –ø—É—Å—Ç–æ–π, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
                if current_chunk:
                    chunks.append(current_chunk)
                
                # –ï—Å–ª–∏ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–æ–ª—å—à–µ max_chunk_size, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –ø–æ —Å–ª–æ–≤–∞–º
                if len(sentence) > max_chunk_size:
                    words = sentence.split()
                    word_chunk = ""
                    
                    for word in words:
                        test_word_chunk = word_chunk + (" " if word_chunk else "") + word
                        if len(test_word_chunk) <= max_chunk_size:
                            word_chunk = test_word_chunk
                        else:
                            if word_chunk:
                                chunks.append(word_chunk)
                            word_chunk = word
                    
                    current_chunk = word_chunk
                else:
                    current_chunk = sentence
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(chunks) > 1 and overlap > 0:
            overlapped_chunks = []
            for i, chunk in enumerate(chunks):
                if i == 0:
                    overlapped_chunks.append(chunk)
                else:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                    prev_chunk = chunks[i-1]
                    
                    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ–ª–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                    words = prev_chunk.split()
                    overlap_words = []
                    overlap_length = 0
                    
                    for word in reversed(words):
                        if overlap_length + len(word) + 1 <= overlap:
                            overlap_words.insert(0, word)
                            overlap_length += len(word) + 1
                        else:
                            break
                    
                    if overlap_words:
                        overlap_text = " ".join(overlap_words)
                        overlapped_chunk = overlap_text + "... " + chunk
                        overlapped_chunks.append(overlapped_chunk)
                    else:
                        overlapped_chunks.append(chunk)
            return overlapped_chunks
        
        return chunks
    
    async def save_summary(
        self, 
        file_path: str, 
        summary: str, 
        file_type: str = "audio",
        metadata: Optional[Dict] = None,
        use_chunks: bool = True
    ) -> List[str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—ã–∂–∏–º–∫—É –≤ Qdrant.
        
        Args:
            file_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            summary: –¢–µ–∫—Å—Ç –≤—ã–∂–∏–º–∫–∏
            file_type: –¢–∏–ø —Ñ–∞–π–ª–∞ (audio, video, text)
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            use_chunks: –†–∞–∑–±–∏–≤–∞—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            await self._ensure_collection_exists()
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            file_hash = self._generate_file_hash(file_path)
            session_id = str(uuid.uuid4())  # –û–±—â–∏–π ID –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            
            base_payload = {
                "file_path": file_path,
                "file_name": Path(file_path).name,
                "file_hash": file_hash,
                "file_type": file_type,
                "created_at": datetime.now().isoformat(),
                "summary_length": len(summary),
                "session_id": session_id,  # –°–≤—è–∑—å –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
                **(metadata or {})
            }
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–∏–∫–æ–º
            if use_chunks and len(summary) > 300:  # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                chunks = self._split_text_into_chunks(summary)
                print(f"üîÑ –†–∞–∑–±–∏–≤–∞—é –≤—ã–∂–∏–º–∫—É –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤...")
            else:
                chunks = [summary]
                print("üîÑ –°–æ—Ö—Ä–∞–Ω—è—é –≤—ã–∂–∏–º–∫—É —Ü–µ–ª–∏–∫–æ–º...")
            
            points_to_save = []
            point_ids = []
            
            for i, chunk in enumerate(chunks):
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
                print(f"üîÑ –°–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞ {i+1}/{len(chunks)}...")
                embedding = await self._get_text_embedding(chunk)
                
                point_id = str(uuid.uuid4())
                point_ids.append(point_id)
                
                payload = {
                    **base_payload,
                    "chunk_text": chunk,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "is_chunk": len(chunks) > 1,
                    "full_summary": summary if len(chunks) == 1 else None  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ—á–∞–Ω–∫–æ–≤–∞–Ω–Ω—ã—Ö
                }
                
                point = PointStruct(
                    id=point_id,
                    vector=embedding,
                    payload=payload
                )
                points_to_save.append(point)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
            await self.client.upsert(
                collection_name=self.collection_name,
                points=points_to_save
            )
            
            if len(chunks) > 1:
                print(f"‚úÖ –í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ Qdrant (session: {session_id})")
            else:
                print(f"‚úÖ –í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ Qdrant —Å ID: {point_ids[0]}")
            
            return point_ids
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ Qdrant: {e}")
            raise
    
    async def search_similar_summaries(self, query: str, limit: int = 10, min_score: float = 0.3) -> List[Dict]:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –≤—ã–∂–∏–º–∫–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤)
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.3
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        try:
            query_embedding = await self._get_text_embedding(query)
            
            results = await self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=limit,
                with_payload=True
            )
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            final_results = []
            for result in results:
                if result.score < min_score:
                    continue
                    
                payload = result.payload
                chunk_text = payload.get('chunk_text', payload.get('full_summary', ''))
                
                chunk_result = {
                    "id": result.id,
                    "score": result.score,
                    "file_name": payload.get('file_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                    "file_type": payload.get('file_type', 'unknown'),
                    "file_path": payload.get('file_path', ''),
                    "created_at": payload.get('created_at', ''),
                    "summary_length": payload.get('summary_length', 0),
                    "summary": chunk_text,
                    "chunk_index": payload.get('chunk_index', 0),
                    "is_chunk": payload.get('is_chunk', False),
                    "session_id": payload.get('session_id', result.id)
                }
                final_results.append(chunk_result)
            
            return final_results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            raise
    
    async def check_file_exists(self, file_path: str) -> Optional[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —É–∂–µ –≤—ã–∂–∏–º–∫–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≤—ã–∂–∏–º–∫–µ –∏–ª–∏ None
        """
        try:
            file_hash = self._generate_file_hash(file_path)
            
            results = await self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {
                            "key": "file_hash",
                            "match": {"value": file_hash}
                        }
                    ]
                },
                limit=100,  # –ë–æ–ª—å—à–µ –ª–∏–º–∏—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
                with_payload=True
            )
            
            if results[0]:  # results = (points, next_page_offset)
                points = results[0]
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
                if len(points) > 1:
                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É
                    sorted_points = sorted(points, key=lambda p: p.payload.get('chunk_index', 0))
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
                    chunk_texts = []
                    session_id = sorted_points[0].payload.get('session_id')
                    
                    for point in sorted_points:
                        chunk_text = point.payload.get('chunk_text', '')
                        if chunk_text:
                            chunk_texts.append(chunk_text)
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏, —É–±–∏—Ä–∞—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                    full_summary = self._reconstruct_from_chunks(chunk_texts)
                    
                    return {
                        "id": session_id,
                        "summary": full_summary,
                        "chunks_count": len(points),
                        "is_chunked": True,
                        **sorted_points[0].payload
                    }
                else:
                    # –û–¥–∏–Ω —á–∞–Ω–∫ –∏–ª–∏ –Ω–µ—á–∞–Ω–∫–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                    point = points[0]
                    summary = point.payload.get('full_summary') or point.payload.get('chunk_text', '')
                    
                    return {
                        "id": point.id,
                        "summary": summary,
                        "chunks_count": 1,
                        "is_chunked": False,
                        **point.payload
                    }
            
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–π–ª–∞: {e}")
            return None
    
    def _reconstruct_from_chunks(self, chunk_texts: List[str]) -> str:
        """
        –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤, —É–±–∏—Ä–∞—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏.
        
        Args:
            chunk_texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —á–∞–Ω–∫–æ–≤
            
        Returns:
            –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not chunk_texts:
            return ""
        
        if len(chunk_texts) == 1:
            return chunk_texts[0]
        
        result = chunk_texts[0]
        
        for i in range(1, len(chunk_texts)):
            current_chunk = chunk_texts[i]
            
            # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ (–∏—â–µ–º –æ–±—â—É—é —á–∞—Å—Ç—å)
            if "..." in current_chunk:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –±–µ—Ä–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –Ω–µ–≥–æ
                parts = current_chunk.split("...", 1)
                if len(parts) > 1:
                    new_part = parts[1].strip()
                    result += ". " + new_part
                else:
                    result += ". " + current_chunk
            else:
                result += ". " + current_chunk
        
        return result
    
 