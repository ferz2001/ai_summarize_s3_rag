import os
from typing import List, Dict, Optional, Any
from datetime import datetime
import uuid
import hashlib
from pathlib import Path

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_qdrant import QdrantVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings.base import Embeddings
from qdrant_client import AsyncQdrantClient, QdrantClient
from qdrant_client.models import Distance, VectorParams

from core.config import config
from services.local_models_service import LocalModelsService


class LocalModelsEmbeddings(Embeddings):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Local Models API."""
    
    def __init__(self, base_url: str, model: str):
        self.base_url = base_url
        self.model = model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        import requests
        
        url = f"{self.base_url}/embeddings"
        payload = {
            "model": self.model,
            "input": texts
        }
        
        try:
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            
            data = response.json()
            if "data" in data:
                return [item["embedding"] for item in data["data"]]
            else:
                raise ValueError("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    
    def embed_query(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
        embeddings = self.embed_documents([text])
        return embeddings[0]
    
    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        return self.embed_documents(texts)
    
    async def aembed_query(self, text: str) -> List[float]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
        return self.embed_query(text)


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ RAG."""
    
    def __init__(self):
        self.async_client = AsyncQdrantClient(
            host=config.QDRANT_HOST,
            port=config.QDRANT_PORT
        )
        self.collection_name = f"{config.QDRANT_COLLECTION_NAME}"
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if config.USE_LOCAL_MODELS:
            self.local_models = LocalModelsService()
            self.embeddings_model = LocalModelsEmbeddings(
                base_url=config.LOCAL_MODELS_URL,
                model=config.LOCAL_EMBEDDING_MODEL
            )
            self.embedding_dim = 1024
            print(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {config.LOCAL_EMBEDDING_MODEL}")
        else:
            self.embeddings_model = OpenAIEmbeddings(
                openai_api_key=config.OPENAI_API_KEY,
                model="text-embedding-3-small"
            )
            self.embedding_dim = 1536
            print(f"üåê –ò—Å–ø–æ–ª—å–∑—É—é OpenAI —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: text-embedding-3-small")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–ª–∏—Ç—Ç–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""],
            keep_separator=False
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vector_store = self._init_vector_store()
    
    def _init_vector_store(self) -> QdrantVectorStore:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ LangChain."""
        client = QdrantClient(
            host=config.QDRANT_HOST,
            port=config.QDRANT_PORT
        )
        
        try:
            collection_info = client.get_collection(self.collection_name)
            existing_size = collection_info.config.params.vectors.size
            if existing_size != self.embedding_dim:
                print(f"üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}' - –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å ({existing_size} -> {self.embedding_dim})")
                client.delete_collection(self.collection_name)
                client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.embedding_dim,
                        distance=Distance.COSINE
                    )
                )
                print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞")
            else:
                print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        except Exception:
            print(f"üîÑ –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}'...")
            client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_dim,
                    distance=Distance.COSINE
                )
            )
            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
        
        return QdrantVectorStore(
            client=client,
            collection_name=self.collection_name,
            embedding=self.embeddings_model
        )
    
    def _generate_file_hash(self, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        file_path = Path(file_path)
        
        if file_path.exists():
            stat = file_path.stat()
            hash_string = f"{file_path.name}_{stat.st_size}_{stat.st_mtime}"
        else:
            hash_string = str(file_path)
        
        return hashlib.md5(hash_string.encode()).hexdigest()
    
    async def _ensure_collection_exists(self):
        """–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        try:
            await self.async_client.get_collection(self.collection_name)
        except Exception:
            print(f"üîÑ –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}'...")
            await self.async_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=self.embedding_dim,
                    distance=Distance.COSINE
                )
            )
    
    async def check_document_exists(self, file_path: str) -> Optional[Dict]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Å–∏—Å—Ç–µ–º–µ."""
        try:
            file_hash = self._generate_file_hash(file_path)
            
            results = await self.async_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {
                            "key": "file_hash",
                            "match": {"value": file_hash}
                        }
                    ]
                },
                limit=100,
                with_payload=True
            )
            
            if results[0]:
                points = results[0]
                
                if len(points) > 1:
                    # –ù–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    sorted_points = sorted(points, key=lambda p: p.payload.get('chunk_index', 0))
                    
                    full_content = " ".join([
                        point.payload.get('page_content', '') 
                        for point in sorted_points
                    ])
                    
                    return {
                        "id": sorted_points[0].payload.get('session_id'),
                        "content": full_content,
                        "chunks_count": len(points),
                        "is_chunked": True,
                        **sorted_points[0].payload
                    }
                else:
                    # –û–¥–∏–Ω —á–∞–Ω–∫
                    point = points[0]
                    return {
                        "id": point.id,
                        "content": point.payload.get('page_content', ''),
                        "chunks_count": 1,
                        "is_chunked": False,
                        **point.payload
                    }
            
            return None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return None
    
    async def add_summary_to_rag(
        self,
        summary_text: str,
        original_text: str,
        file_path: str,
        file_type: str = "summary",
        metadata: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –≤—ã–∂–∏–º–∫—É –≤ RAG —Å–∏—Å—Ç–µ–º—É."""
        await self._ensure_collection_exists()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
        existing = await self.check_document_exists(file_path)
        if existing:
            return {
                "status": "exists",
                "message": f"–î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ —Å–∏—Å—Ç–µ–º–µ",
                "document_id": existing.get("id"),
                "file_path": file_path
            }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        file_hash = self._generate_file_hash(file_path)
        session_id = str(uuid.uuid4())
        
        base_metadata = {
            "file_path": file_path,
            "file_name": Path(file_path).name,
            "file_hash": file_hash,
            "file_type": file_type,
            "created_at": datetime.now().isoformat(),
            "content_length": len(summary_text),
            "original_length": len(original_text),
            "session_id": session_id,
            "is_summary": True,
            **(metadata or {})
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –≤—ã–∂–∏–º–∫—É –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_text(summary_text)
        print(f"üîÑ –†–∞–∑–±–∏–≤–∞—é –≤—ã–∂–∏–º–∫—É –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã LangChain
        documents = []
        for i, chunk in enumerate(chunks):
            doc_metadata = {
                **base_metadata,
                "chunk_index": i,
                "total_chunks": len(chunks),
                "is_chunk": len(chunks) > 1
            }
            
            doc = Document(
                page_content=chunk,
                metadata=doc_metadata
            )
            documents.append(doc)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        ids = self.vector_store.add_documents(documents)
        
        print(f"‚úÖ –í—ã–∂–∏–º–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ RAG ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
        
        return {
            "status": "added",
            "message": f"–í—ã–∂–∏–º–∫–∞ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ RAG —Å–∏—Å—Ç–µ–º—É",
            "document_ids": ids,
            "file_path": file_path,
            "chunks_count": len(ids),
            "summary_length": len(summary_text),
            "original_length": len(original_text)
        }
    
    def search_documents(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.3
    ) -> List[Document]:
        """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."""
        retriever = self.vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": k,
                "score_threshold": score_threshold
            }
        )
        
        documents = retriever.invoke(query)
        return documents
    
    async def generate_answer(
        self,
        question: str,
        context_docs: List[Document],
        max_context_length: int = 3000
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        current_length = 0
        
        for doc in context_docs:
            content = doc.page_content
            if current_length + len(content) <= max_context_length:
                file_name = doc.metadata.get('file_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
                context_parts.append(f"[–ò–∑ —Ñ–∞–π–ª–∞: {file_name}]\n{content}")
                current_length += len(content)
            else:
                break
        
        context = "\n\n---\n\n".join(context_parts)
        
        prompt = f"""–¢—ã —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
            –ö–æ–Ω—Ç–µ–∫—Å—Ç:
            {context}

            –í–æ–ø—Ä–æ—Å: {question}

            –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
            - –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            - –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ
            - –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
            - –ï—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ, —É–∫–∞–∂–∏ –∏–∑ –∫–∞–∫–æ–≥–æ —Ñ–∞–π–ª–∞ –≤–∑—è—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

            –û—Ç–≤–µ—Ç:
        """

        if config.USE_LOCAL_MODELS:
            response = await self.local_models.chat_completion([
                {
                    "role": "system",
                    "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ], model=config.LOCAL_CHAT_MODEL)
            return response
        else:
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                openai_api_key=config.OPENAI_API_KEY,
                model="gpt-4o-mini",
                temperature=0.3
            )
            response = await llm.ainvoke(prompt)
            return response.content
    
    async def query(
        self,
        question: str,
        k: int = 5,
        score_threshold: float = 0.3
    ) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è RAG –∑–∞–ø—Ä–æ—Å–æ–≤."""
        print(f"üîç RAG –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{question}'")
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = self.search_documents(question, k=k, score_threshold=score_threshold)
        
        if not documents:
            return {
                "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.",
                "sources": [],
                "context_used": False
            }
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        answer = await self.generate_answer(question, documents)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        sources = []
        for doc in documents:
            sources.append({
                "file_name": doc.metadata.get('file_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'),
                "file_type": doc.metadata.get('file_type', 'unknown'),
                "chunk_index": doc.metadata.get('chunk_index', 0),
                "content_preview": doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
            })
        
        return {
            "answer": answer,
            "sources": sources,
            "context_used": True,
            "documents_found": len(documents)
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
rag_service = RAGService() 